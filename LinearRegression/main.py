import numpy as np
import matplotlib.pyplot as plt

'''
1. æ¨¡å‹å…¬å¼
2. è®­ç»ƒæ•°æ®ï¼Œéšæœºå‚æ•°é¢„æµ‹å€¼
3. æŸå¤±å‡½æ•°æ±‚é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„å·®å¼‚
4. å¯¹æŸå¤±å‡½æ•°æ±‚å¯¼ï¼Œæ±‚å‡ºå‚æ•°å˜ä¸ºä¸æŸå¤±å€¼çš„å…³ç³»
5. æ ¹æ®å¯¼æ•°æ›´æ–°å‚æ•°ï¼Œä½¿å¾—æŸå¤±å€¼è¶Šæ¥è¶Šå°ï¼Œé¢„æµ‹å€¼æ¥è¿‘çœŸå®å€¼ï¼Œæ¬¡æ•°çš„å‚æ•°é€¼è¿‘æœ€ä¼˜è§£
'''
class LinearRegression:
    def __init__(self, lr=0.0001, thr=1e-6):
        self.lr = lr
        self.thr = thr

    def predict(self, X):
        paddedX = np.ones([X.shape[0], X.shape[1] + 1])
        paddedX[:, :-1] = X
        return paddedX @ self.theta  # è®¡ç®—é¢„æµ‹å€¼

    def fit(self, X, y):
        '''

        Args:
            X:
            y:
                    X = [
                        [1.2],
                        [3.4],
                        [5.6],
                        ...,
                        [9.8]
                    ]
        Returns:

        '''
        # æ„é€ theta
        '''
        éšæœº
        theta = [
            [0.3],  # æƒé‡
            [0.5]   # åç½®
        ]
        '''
        self.theta = np.random.randn(X.shape[-1] + 1)[:, np.newaxis]
        # æ„é€ æ‰©å±•çš„X
        '''
        paddedX = [
            [1.2, 1],
            [3.4, 1],
            [5.6, 1],
            ...,
            [9.8, 1]
        ]
        '''
        paddedX = np.ones([X.shape[0], X.shape[1] + 1])
        paddedX[:, :-1] = X
        # å¼€å§‹è¿›è¡Œæ¢¯åº¦ä¸‹é™
        epoch = 0
        while True:
            # è®¡ç®—loss
            # paddedX @ self.theta  y=wâ‹…x+b y=XQ
            '''
            y_pred = paddedX @ theta = [
                [25.6],
                [72.4],
                [119.2],
                ...,
                [200.1]
            ]
            paddedX æ˜¯æ‰©å±•åçš„ç‰¹å¾çŸ©é˜µï¼Œæ¯è¡Œä»£è¡¨ä¸€ä¸ªæ ·æœ¬ï¼Œæœ€åä¸€åˆ—ä¸ºå¸¸æ•° 1ï¼Œä¸“é—¨ç”¨äºè®¡ç®—åç½®é¡¹
            self.theta æ˜¯å‚æ•°å‘é‡ï¼ŒåŒ…å«æƒé‡w å’Œåç½®b
            ç‰¹å¾éƒ¨åˆ†: æ¯ä¸€è¡Œçš„ç‰¹å¾å€¼ä¸å¯¹åº”çš„æƒé‡ç›¸ä¹˜ 
            åç½®éƒ¨åˆ†: å¸¸æ•° 1 å’Œåç½®b ç›¸ä¹˜ï¼Œç›´æ¥åŠ åˆ°ç»“æœä¸­
            æ ·æœ¬çŸ©é˜µ paddedX çš„ä¸€è¡Œæ˜¯ [x1,x2,1] å‚æ•°å‘é‡ self.theta æ˜¯[w1,w2,b]
            çŸ©é˜µä¹˜æ³•ï¼špaddedX @ self.theta =  [x1,x2,1] @ [w1,w2,b] = w1*x1 + w2*x2 + b 
            @ ç‚¹ç§¯å°±æ˜¯w1*x1 + w2*x2 + b 
            '''
            loss = 0.5 * np.mean((paddedX @ self.theta - y) ** 2)
            print("ç¬¬{}æ¬¡è®­ç»ƒï¼Œlosså¤§å°ä¸º{}".format(epoch, loss))
            # è®¡ç®—æ¢¯åº¦
            '''
            grad = paddedX.T @ (paddedX @ self.theta - y)
            è®¡ç®—æŸå¤±å‡½æ•°å¯¹ Î¸ çš„å¯¼æ•°
            å¤šå…ƒçº¿æ€§å›å½’æ¨¡å‹çš„æŸå¤±å‡½æ•°å¯¹å‚æ•°çš„å¯¼æ•° 
            å‡è®¾å¤šå…ƒçº¿æ€§å›å½’æ¨¡å‹çš„è¡¨è¾¾å¼ä¸ºï¼šy=XÎ¸ 
            å…¶ä¸­ï¼šy æ˜¯ç›®æ ‡å€¼çš„å‘é‡ï¼Œå½¢çŠ¶ä¸º(m,1)ï¼Œm æ˜¯æ ·æœ¬æ•°é‡ã€‚
            X æ˜¯è®¾è®¡çŸ©é˜µ å½¢çŠ¶ä¸º (m,n)ï¼Œæ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªæ ·æœ¬çš„ç‰¹å¾å‘é‡ï¼Œn æ˜¯ç‰¹å¾æ•°é‡ã€‚
            Î¸ æ˜¯æƒé‡å‚æ•°å‘é‡ å½¢çŠ¶ä¸º (n,1)ã€‚
            X@Î¸ æ˜¯æ¨¡å‹çš„é¢„æµ‹å€¼ï¼ˆå½¢çŠ¶ä¸º (m,1)ï¼‰ã€‚
            æŸå¤±å‡½æ•°: å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰L(Î¸)= 1/2m |XÎ¸âˆ’y| XÎ¸âˆ’y æ˜¯è¯¯å·®ï¼ˆæ®‹å·®ï¼‰âˆ¥â‹…âˆ¥ 2æ˜¯å‘é‡çš„å¹³æ–¹å’Œã€‚
            ä¸ºäº†ä¼˜åŒ–æ¨¡å‹å‚æ•° Î¸ï¼Œæˆ‘ä»¬å¯¹æŸå¤±å‡½æ•° ğ¿(ğœƒ) å¯¹ Î¸ æ±‚å¯¼ã€‚å¯¹çŸ©é˜µè¿ç®—æ±‚å¯¼çš„ç»“æœä¸ºï¼šâˆ‚L(Î¸)/âˆ‚Î¸ = 1/m xT(XÎ¸âˆ’y) å…¶ä¸­âˆ‚Î¸æ˜¯å¯¼æ•°ã€æ¢¯åº¦ æ±‚è§£å‡ºæ¥æ›´æ–°å‚æ•°ï¼Œâˆ‚Î¸çš„å˜åŒ–å¯¹å‚æ•°çš„å½±å“ï¼Œä½¿å¾—æŸå¤±å€¼æœ€å°
            '''
            grad = paddedX.T @ (paddedX @ self.theta - y)
            # æ˜¯å¦æ”¶æ•›
            if abs(np.sum(grad)) < self.thr:
                break
            # æ¢¯åº¦ä¸‹é™
            self.theta -= self.lr * grad
            epoch += 1


if __name__ == "__main__":
    data_size = 100
    # å¦‚ x = [1.2, 3.4, 5.6, ..., 9.8]
    x = np.random.uniform(low=1.0, high=10.0, size=data_size)
    # å› å˜é‡ï¼ˆç›®æ ‡å€¼ï¼‰ï¼Œé€šè¿‡ y = x * 20 + 10 + noise ç”Ÿæˆï¼Œå…¶ä¸­ noise æ˜¯æ­£æ€åˆ†å¸ƒçš„å™ªå£° y = [35.6, 78.3, 122.1, ..., 210.4]
    y = x * 20 + 10 + np.random.normal(loc=0.0, scale=10.0, size=data_size)
    lr = LinearRegression()
    # å°† x ä»ä¸€ç»´å‘é‡ï¼ˆå½¢çŠ¶ (100,)ï¼‰å˜ä¸ºäºŒç»´åˆ—å‘é‡ï¼ˆå½¢çŠ¶ (100, 1)ï¼‰ã€‚
    '''
    x = [1.2, 3.4, 5.6, ..., 9.8]
    X = [
        [1.2],
        [3.4],
        [5.6],
        ...,
        [9.8]
    ]
    '''
    lr.fit(x[:, np.newaxis], y[:, np.newaxis])
    plt.scatter(x, y, marker='.')
    plt.plot([1, 10], [lr.theta[0] * 1 + lr.theta[1], lr.theta[0] * 10 + lr.theta[1]], "m-")
    plt.show()


'''
# å‡è®¾ paddedX æ˜¯ï¼š
# [[x1_1, x1_2, 1],
#  [x2_1, x2_2, 1],
#  ...]
# self.theta æ˜¯ï¼š
# [[w1],
#  [w2],
#  [b]]

# ç»“æœé¢„æµ‹å€¼ y_hat:
y_hat = paddedX @ self.theta

# y_hat çš„æ¯ä¸€è¡Œç»“æœè¡¨ç¤ºæ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹å€¼ï¼š
# [[x1_1 * w1 + x1_2 * w2 + b],
#  [x2_1 * w1 + x2_2 * w2 + b],
#  ...]
'''
