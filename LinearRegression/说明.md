# 线性回归公式原理与梯度下降法

## 1. 线性回归公式
线性回归模型的目标是找到一条直线（高维情况下是超平面），其公式为：

\[
y = w \cdot x + b
\]

- \(x\): 输入特征（可以是一维或多维向量）。
- \(w\): 权重向量（参数，表示每个特征的影响程度）。
- \(b\): 偏置（参数，表示线性模型的截距）。
- \(y\): 模型预测值。

## 2. 目标函数（损失函数）
我们使用 **均方误差（Mean Squared Error, MSE）** 来衡量预测值与真实值之间的差距，其公式为：

\[
\text{Loss} = \frac{1}{2N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2
\]

- \(N\): 数据样本的数量。
- \(y_i\): 第 \(i\) 个样本的真实值。
- \(\hat{y}_i\): 第 \(i\) 个样本的预测值。

## 3. 梯度下降法
梯度下降法通过迭代优化参数 \(w\) 和 \(b\) 来最小化损失函数。

### 3.1 损失函数的梯度
1. 对权重 \(w\) 的梯度：
   \[
   \frac{\partial \text{Loss}}{\partial w} = -\frac{1}{N} \sum_{i=1}^{N} x_i \cdot (y_i - \hat{y}_i)
   \]
2. 对偏置 \(b\) 的梯度：
   \[
   \frac{\partial \text{Loss}}{\partial b} = -\frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)
   \]

在矩阵形式中，梯度可以表示为：
\[
\text{grad} = X^T \cdot (X \cdot \theta - y)
\]
- \(X\): 特征矩阵，包含输入特征和偏置项。
- \(\theta = [w, b]\): 模型参数向量。

### 3.2 参数更新公式
每次迭代更新参数：
\[
\theta = \theta - \eta \cdot \text{grad}
\]

- \(\eta\): 学习率（控制每次更新的步长）。
- \(\text{grad}\): 梯度，表示损失函数对参数的变化率。

---

## 样例：梯度下降法的计算过程

### 样例数据
假设我们有以下数据点：

| \(x\) | \(y\) |
|-------|-------|
| 1.0   | 3.0   |
| 2.0   | 5.0   |
| 3.0   | 7.0   |

目标：用梯度下降法找到一条直线 \(y = w \cdot x + b\)，使预测值 \(\hat{y}\) 和真实值 \(y\) 的误差最小化。

### 初始化参数
- 权重 \(w = 0.5\)
- 偏置 \(b = 0.5\)
- 学习率 \(\eta = 0.1\)

---

### 第一步：计算预测值
公式：
\[
\hat{y} = w \cdot x + b
\]

代入数据，计算得到：
- 对于 \(x = 1.0\): \(\hat{y} = 0.5 \cdot 1.0 + 0.5 = 1.0\)
- 对于 \(x = 2.0\): \(\hat{y} = 0.5 \cdot 2.0 + 0.5 = 1.5\)
- 对于 \(x = 3.0\): \(\hat{y} = 0.5 \cdot 3.0 + 0.5 = 2.0\)

预测值为：\(\hat{y} = [1.0, 1.5, 2.0]\)。

---

### 第二步：计算损失值
损失函数：
\[
\text{Loss} = \frac{1}{2N} \sum_{i=1}^{N} (\hat{y}_i - y_i)^2
\]

代入数据，计算得到：
\[
\text{Loss} = \frac{1}{6} \left[(1.0 - 3.0)^2 + (1.5 - 5.0)^2 + (2.0 - 7.0)^2 \right]
\]
\[
\text{Loss} = \frac{1}{6} \left[4.0 + 12.25 + 25.0 \right] = \frac{41.25}{6} \approx 6.875
\]

---

### 第三步：计算梯度
1. 对权重 \(w\) 的梯度：
   \[
   \frac{\partial \text{Loss}}{\partial w} = -\frac{1}{3} \left[1.0 \cdot (3.0 - 1.0) + 2.0 \cdot (5.0 - 1.5) + 3.0 \cdot (7.0 - 2.0)\right]
   \]
   \[
   \frac{\partial \text{Loss}}{\partial w} = -\frac{1}{3} [2.0 + 7.0 + 15.0] = -8.0
   \]

2. 对偏置 \(b\) 的梯度：
   \[
   \frac{\partial \text{Loss}}{\partial b} = -\frac{1}{3} \left[(3.0 - 1.0) + (5.0 - 1.5) + (7.0 - 2.0)\right]
   \]
   \[
   \frac{\partial \text{Loss}}{\partial b} = -\frac{1}{3} [2.0 + 3.5 + 5.0] = -3.5
   \]

---

### 第四步：更新参数
更新公式：
\[
\theta = \theta - \eta \cdot \text{grad}
\]

- 更新权重 \(w\):
  \[
  w = 0.5 - 0.1 \cdot (-8.0) = 1.3
  \]
- 更新偏置 \(b\):
  \[
  b = 0.5 - 0.1 \cdot (-3.5) = 0.85
  \]

新参数：\(w = 1.3, b = 0.85\)。

---

### 第五步：迭代
重复上述步骤，直到损失函数收敛为最小值。

---

## 可视化结果
最终，线性回归模型拟合出了一条直线，展示了训练数据和拟合结果：
- 蓝点：训练数据。
- 粉色线：拟合出的回归线。
